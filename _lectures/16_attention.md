---
type: lecture
date: 2026-03-24T15:30:00-5:00
title: 16 - Attention and Transformers
tldr: "In this lecture we cover the attention mechanism used to handle longer variable length contexts, and present the transformer architure, self-attention and the variations for encoder, decoder and encoder-decoder type models."
#thumbnail: /_images/thumbnails/ds598-lecture-12.png
hide_from_announcments: true
links:
    - url: /static_files/lectures/16_attention_and_transformers.pdf
      name: slides
---
**Readings:**
- [Understanding Deep Learning](https://github.com/udlbook/udlbook/releases/download/v5.0.2/UnderstandingDeepLearning_05_29_25_C.pdf), Chapter 12
- Optional: [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- Optional: [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
