---
type: lecture
date: 2025-10-27T14:30:00-5:00
title: 16 - Attention and Transformers
tldr: "In this lecture we cover the attention mechanism used to handle longer variable length contexts, and present the transformer architure, self-attention and the variations for encoder, decoder and encoder-decoder type models."
#thumbnail: /_images/thumbnails/ds598-lecture-12.png
hide_from_announcments: true
links:
    - url: /static_files/lectures/16_attention.pdf
      name: slides
    - url: /static_files/lectures/16_attention_annotated.pdf
      name: annotated slides
    - url: https://echo360.org/lesson/G_838b35d7-329e-402a-876b-d51a5a4e2e38_ce95771d-32af-4a00-a68f-ff891306f848_2025-10-27T14:30:00.000_2025-10-27T16:15:00.000/classroom
      name: lecture recording
---
**Readings:**
- [Understanding Deep Learning](https://github.com/udlbook/udlbook/releases/download/v5.0.2/UnderstandingDeepLearning_05_29_25_C.pdf), Chapter 12
- Optional: [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- Optional: [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
