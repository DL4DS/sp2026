{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMfLrTH4aGl2"
   },
   "source": [
    "# CS 542 Fall 2025 Homework 4\n",
    "\n",
    "Your task for this homework is to adapt this notebook to run on the Shared Compute Cluster (SCC).\n",
    "\n",
    "## Background\n",
    "\n",
    "This notebook builds a model detecting trees in images.\n",
    "The data set consists of roughly 500 pictures and is currently stored in `/projectnb/ds542/materials/tree-or-not` on the SCC.\n",
    "Most of them are from the Boston area, but some are from around the globe.\n",
    "Most of them were taken outside, but some were taken inside or in more exotic locations.\n",
    "Many other factors such as lighting, weather, and confounding bushes will make this a challenging problem.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Copy this notebook to your class directory on the SCC, i.e. `/projectnb/ds542/students/USERNAME`.\n",
    "2. Run your copy notebook in Jupyter on the SCC using at least one GPU as explained in the SCC lecture.\n",
    "3. Confirm that you have GPU access using the code below.\n",
    "4. Modify the data loading code to read from the shared directory `/projectnb/ds542/materials/tree-or-not`.\n",
    "5. Run the model training code. Make sure that it is running on GPU, not CPU.\n",
    "6. Answer the question at the bottom of this notebook.\n",
    "7. Submit just this notebook to Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules\n",
    "\n",
    "Run but do not change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5hXzVgnaGl9"
   },
   "outputs": [],
   "source": [
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Access\n",
    "\n",
    "Run but do not change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QW0hW-araGl-",
    "outputId": "0a5c2011-8a4b-46e9-e612-c1a62eb8c5f2"
   },
   "outputs": [],
   "source": [
    "def to_gpu(t):\n",
    "    if torch.cuda.is_available():\n",
    "        return t.cuda()\n",
    "    return t\n",
    "\n",
    "def to_numpy(t):\n",
    "    return t.detach().cpu().numpy()\n",
    "\n",
    "device = to_gpu(torch.ones(1,1)).device\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "This code originally relied on checking out or cloning [this repository](https://github.com/dl4ds/fa2024_midterm).\n",
    "\n",
    "Modify this code to load from `/projectnb/ds542/materials/tree-or-not`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLnleFagaGmA",
    "outputId": "fe6b43d0-85e8-40c0-a6b0-6d7abdef6405"
   },
   "outputs": [],
   "source": [
    "# the repository has images scaled to standard widths of 64, 128 and 256.\n",
    "# you may use larger images if you prefer.\n",
    "\n",
    "image_width = 64\n",
    "\n",
    "def load_data_set(data_set_name):\n",
    "    labels = pd.read_csv(f\"fa2024_midterm/{data_set_name}.tsv\", sep=\"\\t\")\n",
    "\n",
    "    file_names = []\n",
    "    images = []\n",
    "    targets = []\n",
    "    for i in range(labels.shape[0]):\n",
    "        row = labels.iloc[i]\n",
    "        try:\n",
    "            image = imageio.imread(f\"fa2024_midterm/images{image_width}/{row['filename']}\")[...]\n",
    "        except:\n",
    "            print(\"SKIPPING \", row['filename'], \"MISSING\")\n",
    "            continue\n",
    "\n",
    "        if image.shape[0] != image.shape[1] * 3 // 4:\n",
    "            print(\"SKIPPING \", row['filename'], image.shape)\n",
    "            continue\n",
    "\n",
    "        # convert from 0-255 to 0.0-1.0\n",
    "        image = image / 255\n",
    "        # prepend axis with length one\n",
    "        # image = image.reshape(1, *image.shape)\n",
    "        image = torch.tensor(image, device=device, dtype=torch.float32)\n",
    "        # permute image dimensions to put color channel first\n",
    "        image = torch.permute(image, [2, 0, 1])\n",
    "\n",
    "        file_names.append(row['filename'])\n",
    "        images.append(image)\n",
    "        targets.append(row[\"target\"])\n",
    "\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    targets = torch.tensor(targets, device=device, dtype=torch.float32)\n",
    "    targets = targets.long()\n",
    "\n",
    "    return (file_names, images, targets)\n",
    "\n",
    "train_data_set = load_data_set(\"train\")\n",
    "for t in train_data_set[1:]:\n",
    "    print(\"TRAIN\", t.shape, t.dtype, t.device)\n",
    "(train_file_names, train_X, train_Y) = train_data_set\n",
    "\n",
    "validation_data_set = load_data_set(\"validation\")\n",
    "for t in validation_data_set[1:]:\n",
    "    print(\"VALIDATION\", t.shape, t.dtype, t.device)\n",
    "(validation_file_names, validation_X, validation_Y) = validation_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fIXCLGqaGmB",
    "outputId": "87b65cc1-b05a-4a48-afb0-0470b0cadd3a"
   },
   "outputs": [],
   "source": [
    "plt.imshow(to_numpy(torch.permute(train_X[0,:,:,:], (1, 2, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-zXXZ4maGl_"
   },
   "source": [
    "## Model Building\n",
    "\n",
    "Do not modify any of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRFJ7RrBaGmB"
   },
   "outputs": [],
   "source": [
    "class TreeNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_0 = torch.nn.Conv2d(in_channels=3, out_channels=5, kernel_size=5, stride=2, device=device)\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=5, out_channels=5, kernel_size=3, stride=2, device=device)\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=5, out_channels=5, kernel_size=1, device=device)\n",
    "        self.fc_3 = torch.nn.Linear(700, 2)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = self.conv_0(X)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        X = self.conv_1(X)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        X = self.conv_2(X)\n",
    "        X = self.relu(X)\n",
    "\n",
    "        # flatten channels and image dimensions\n",
    "        X = X.reshape(X.shape[:-3] + (-1,))\n",
    "\n",
    "        X = self.fc_3(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "test_model = TreeNetwork().to(device)\n",
    "test_output = test_model(train_X[:5,:,:,:])\n",
    "assert test_output.shape == (5, 2)\n",
    "del test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqGF4li0aGmC"
   },
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oByjz-RSaGmC"
   },
   "outputs": [],
   "source": [
    "DEFAULT_EPOCHS = 1000 if torch.cuda.is_available() else 100\n",
    "\n",
    "def train_model(model_class, epochs=DEFAULT_EPOCHS, learning_rate=1e-4, **kwargs):\n",
    "    model = model_class(**kwargs)\n",
    "    try:\n",
    "        model = model.cuda()\n",
    "    except:\n",
    "        print(\"cuda() failed\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        prediction = model(train_X)\n",
    "        loss = loss_function(prediction, train_Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            liveloss_updates = {}\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "\n",
    "                metrics = {}\n",
    "                def get_metrics(metrics_prefix, metrics_X, metrics_Y):\n",
    "                    metrics_prediction = model(metrics_X)\n",
    "\n",
    "                    return {\n",
    "                        f\"{metrics_prefix}loss\": loss_function(metrics_prediction, metrics_Y)\n",
    "                    }\n",
    "\n",
    "                metrics.update(get_metrics(\"train_\", train_X, train_Y))\n",
    "                metrics.update(get_metrics(\"val_\", validation_X, validation_Y))\n",
    "                print(\"ITER\", i+1, \"METRICS\", metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XV7wO16oaGmD",
    "outputId": "c00d1c5e-61e3-4da2-e3d0-1ad6f0d60aa2"
   },
   "outputs": [],
   "source": [
    "tree_model = train_model(TreeNetwork, epochs=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Review the metrics printed above while training the model.\n",
    "What trends do you notice in the training and validation losses?\n",
    "Just state the trends that you see.\n",
    "You do not need to explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
