{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DS 542 Fall 2025 Project 3\n",
        "\n",
        "Your task for this project is to train an attention-based decoder-only model for math expressions with positive integers, addition, and parenthesis.\n",
        "A sample model is provided and demonstrated on small problems with single digit integer inputs.\n",
        "Your goal is to scale up this model to handle two digit inputs and longer expressions."
      ],
      "metadata": {
        "id": "VrGSUTn7gNgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Setup"
      ],
      "metadata": {
        "id": "LXZzLTrSjvWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "KFOPSuqcM5z8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqsmhQIfUEJS",
        "outputId": "cabc900b-e5e0-476d-fe9c-f964604e0d5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "characters = \"()+0123456789=\"\n",
        "TOKENS = [\"<bos>\", \"<eos>\", \"<pad>\"] + [c for c in characters]\n",
        "print(TOKENS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGkixl9qUG7b",
        "outputId": "2e5848f7-8abc-4d04-ee6e-19caaff67531"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<bos>', '<eos>', '<pad>', '(', ')', '+', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOKEN_MAP = dict((t, i) for i, t in enumerate(TOKENS))\n",
        "print(TOKEN_MAP)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9c-gKxIUctq",
        "outputId": "6d530d33-d556-48fd-c9de-1658a9a67d2f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<bos>': 0, '<eos>': 1, '<pad>': 2, '(': 3, ')': 4, '+': 5, '0': 6, '1': 7, '2': 8, '3': 9, '4': 10, '5': 11, '6': 12, '7': 13, '8': 14, '9': 15, '=': 16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BOS = TOKEN_MAP[\"<bos>\"]\n",
        "EOS = TOKEN_MAP[\"<eos>\"]\n",
        "PAD = TOKEN_MAP[\"<pad>\"]"
      ],
      "metadata": {
        "id": "epg4G8WnU_PT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(token_ids):\n",
        "    return \"\".join(TOKENS[i] for i in token_ids)\n",
        "\n",
        "decode([0, 3, 7, 5, 6, 4, 1, 2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XB6-LzeLUsgZ",
        "outputId": "cfa5839d-ee91-429a-8a56-0bb77d9295a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos>(1+0)<eos><pad>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(s, *, eos=True):\n",
        "    if s.startswith(\"<bos>\"):\n",
        "        s = s[5:]\n",
        "\n",
        "    output = [BOS]\n",
        "    output.extend(TOKEN_MAP[c] for c in s)\n",
        "\n",
        "    if eos:\n",
        "        output.append(EOS)\n",
        "\n",
        "    return torch.tensor(output, device=device)\n",
        "\n",
        "decode(encode(\"1+2=3\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BoIhIZ7TVNTZ",
        "outputId": "cab059f3-f0b9-429a-f4f2-b6d74556bb87"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos>1+2=3<eos>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Generation\n",
        "\n",
        "This function `generate_instance` will generate a random expression starting from `n` random integers between `value_min` and `value_max` (inclusive) and combining them with addition in a random order.\n",
        "The full expression consists of multiple rounds of reductions of the innermost parentheses replacing the parenthesized addition with its integer value.\n",
        "The final value after the last equals sign is the value of the original expression before the first equals sign.\n",
        "\n",
        "Here are some example expressions.\n",
        "\n",
        "* `(3+4)+(9+2)=(7+11)=18`\n",
        "* `(((((1+2)+3)+4)+5)+6)=((((3+3)+4)+5)+6)=(((6+4)+5)+6)=((10+5)+6)=(15+6)=21`\n",
        "\n",
        "To be clear, each reduction step should replace all the parenthesis that only contain two numbers being added.\n"
      ],
      "metadata": {
        "id": "pVOr1270kQS7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tLAnoy3qMmyl",
        "outputId": "b95dfc61-acab-4b19-8a5a-5e259e038cce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos>(1+(6+6))=(1+12)=13<eos>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "def generate_instance(n, *, value_min=1, value_max=9):\n",
        "    current_numbers = [random.randint(value_min, value_max) for _ in range(n)]\n",
        "    current_expressions = [[str(v) for v in current_numbers]]\n",
        "    current_fresh = [True for _ in current_numbers]\n",
        "\n",
        "    while len(current_numbers) > 1:\n",
        "        next_numbers = []\n",
        "        next_expressions = [[] for _ in range(len(current_expressions) + 1)]\n",
        "        next_fresh = []\n",
        "\n",
        "        i = 0\n",
        "        while i < len(current_numbers):\n",
        "            can_merge = (i + 1 < len(current_numbers)) and (current_fresh[i] or current_fresh[i + 1])\n",
        "            if can_merge and random.random() < 0.5:\n",
        "                # decided to merge\n",
        "                next_numbers.append(current_numbers[i] + current_numbers[i + 1])\n",
        "\n",
        "                next_expressions[0].append(str(next_numbers[-1]))\n",
        "                for j in range(len(current_expressions)):\n",
        "                    next_expressions[j + 1].append(f\"({current_expressions[j][i]}+{current_expressions[j][i + 1]})\")\n",
        "\n",
        "                next_fresh.append(True)\n",
        "                i += 2\n",
        "            else:\n",
        "                # decided not to merge\n",
        "                next_numbers.append(current_numbers[i])\n",
        "\n",
        "                next_expressions[0].append(str(next_numbers[-1]))\n",
        "                for j in range(len(current_expressions)):\n",
        "                    next_expressions[j + 1].append(current_expressions[j][i])\n",
        "\n",
        "                next_fresh.append(False)\n",
        "                i += 1\n",
        "\n",
        "        if len(next_numbers) < len(current_numbers):\n",
        "            current_numbers = next_numbers\n",
        "            current_expressions = next_expressions\n",
        "            current_fresh = next_fresh\n",
        "\n",
        "    output = '='.join(e[0] for e in reversed(current_expressions))\n",
        "    return encode(output)\n",
        "\n",
        "decode(generate_instance(3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(decode(generate_instance(5)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms7HHwAeVn7s",
        "outputId": "1259f14e-47e0-47d8-a0d2-397b86ba5b01"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos>(((1+(2+5))+9)+1)=(((1+7)+9)+1)=((8+9)+1)=(17+1)=18<eos>\n",
            "<bos>(((4+(2+6))+8)+3)=(((4+8)+8)+3)=((12+8)+3)=(20+3)=23<eos>\n",
            "<bos>(4+(3+(3+(1+5))))=(4+(3+(3+6)))=(4+(3+9))=(4+12)=16<eos>\n",
            "<bos>(9+((8+(3+1))+5))=(9+((8+4)+5))=(9+(12+5))=(9+17)=26<eos>\n",
            "<bos>((((9+4)+9)+1)+5)=(((13+9)+1)+5)=((22+1)+5)=(23+5)=28<eos>\n",
            "<bos>(3+((6+6)+(1+7)))=(3+(12+8))=(3+20)=23<eos>\n",
            "<bos>(4+(3+((2+3)+4)))=(4+(3+(5+4)))=(4+(3+9))=(4+12)=16<eos>\n",
            "<bos>(((2+(8+1))+8)+5)=(((2+9)+8)+5)=((11+8)+5)=(19+5)=24<eos>\n",
            "<bos>(4+((7+8)+(7+4)))=(4+(15+11))=(4+26)=30<eos>\n",
            "<bos>(9+(7+(6+(1+2))))=(9+(7+(6+3)))=(9+(7+9))=(9+16)=25<eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a model that generalizes to more numbers and larger numbers\n"
      ],
      "metadata": {
        "id": "66Kb3SE0ZUzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sample code that follows is based on this ChatGPT session.\n",
        "\n",
        "https://chatgpt.com/share/69036c83-171c-800c-9216-0884476017c6"
      ],
      "metadata": {
        "id": "1mpOqGhGZhsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(*args, batch_size=64, **kwargs):\n",
        "    seqs = [generate_instance(*args, **kwargs) for _ in range(batch_size)]\n",
        "\n",
        "    # pad to max length on right\n",
        "    batch = torch.nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=PAD)\n",
        "\n",
        "    # next token targets: inputs are all but last; targets are all but first\n",
        "    x = batch[:, :-1]\n",
        "    y = batch[:, 1:]\n",
        "\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "make_batch(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxlCyaQiV1zb",
        "outputId": "10182c80-db24-4204-d837-f15950cfd855"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  3,  3,  ...,  2,  2,  2],\n",
              "         [ 0,  3,  3,  ..., 16,  8,  9],\n",
              "         [ 0,  3,  3,  ...,  2,  2,  2],\n",
              "         ...,\n",
              "         [ 0,  3,  3,  ...,  2,  2,  2],\n",
              "         [ 0,  3,  3,  ...,  2,  2,  2],\n",
              "         [ 0,  3, 11,  ..., 16,  8,  6]], device='cuda:0'),\n",
              " tensor([[ 3,  3,  3,  ...,  2,  2,  2],\n",
              "         [ 3,  3,  3,  ...,  8,  9,  1],\n",
              "         [ 3,  3,  3,  ...,  2,  2,  2],\n",
              "         ...,\n",
              "         [ 3,  3,  3,  ...,  2,  2,  2],\n",
              "         [ 3,  3,  3,  ...,  2,  2,  2],\n",
              "         [ 3, 11,  5,  ...,  8,  6,  1]], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(T):\n",
        "    # shape (T, T); True = mask (disallow), False = keep\n",
        "    # nn.Transformer expects float mask or bool depending on API;\n",
        "    # TransformerEncoder uses src_mask where non-zero entries are masked.\n",
        "    # We'll use a float mask with -inf above diagonal.\n",
        "    m = torch.full((T, T), float(\"-inf\"), device=device)\n",
        "    m = torch.triu(m, diagonal=1)  # upper triangle is masked\n",
        "    return m"
      ],
      "metadata": {
        "id": "DU0efN0iWsBs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "class MathTransformer(torch.nn.Module):\n",
        "    def __init__(self, d_model=128, nhead=4, num_layers=4, dim_ff=256, max_len=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        vocab_size = len(TOKENS)\n",
        "\n",
        "        # token + position embeddings\n",
        "        self.tok_emb = torch.nn.Embedding(vocab_size, d_model, padding_idx=PAD)\n",
        "        self.pos_emb = torch.nn.Embedding(max_len, d_model)\n",
        "\n",
        "        layer = torch.nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
        "            dropout=dropout, batch_first=True,\n",
        "        )\n",
        "        self.blocks = torch.nn.TransformerEncoder(layer, num_layers=num_layers)\n",
        "        self.lm_head = torch.nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # init\n",
        "        torch.nn.init.normal_(self.tok_emb.weight, mean=0.0, std=0.02)\n",
        "        torch.nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.02)\n",
        "        torch.nn.init.normal_(self.lm_head.weight, mean=0.0, std=0.02)\n",
        "        torch.nn.init.zeros_(self.lm_head.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, T)\n",
        "        N, T = x.shape\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0)  # (1, T)\n",
        "        h = self.tok_emb(x) * math.sqrt(self.d_model) + self.pos_emb(pos)  # (N, T, d_model)\n",
        "\n",
        "        # key padding mask: True where we want to ignore (PAD)\n",
        "        key_padding_mask = (x == PAD)  # (N, T) bool\n",
        "\n",
        "        # causal mask for self-attention (float, -inf above diagonal)\n",
        "        attn_mask = causal_mask(T) # (T, T)\n",
        "\n",
        "        h = self.blocks(\n",
        "            h,\n",
        "            mask=attn_mask,                         # causal\n",
        "            src_key_padding_mask=key_padding_mask   # pad masking\n",
        "        )\n",
        "        logits = self.lm_head(h)  # (N, T, vocab)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, prefix_ids, max_new_tokens=8):\n",
        "        self.eval()\n",
        "        x = prefix_ids.clone().to(next(self.parameters()).device)  # (N, T0)\n",
        "        for _ in range(max_new_tokens):\n",
        "            if x.size(1) >= 64:\n",
        "                break\n",
        "            logits = self.forward(x)[:, -1, :]   # (N, V)\n",
        "            next_id = torch.argmax(logits, dim=-1, keepdim=True)  # greedy\n",
        "            x = torch.cat([x, next_id], dim=1)\n",
        "            if (next_id == EOS).all():\n",
        "                break\n",
        "        return x\n",
        "\n",
        "test_model = MathTransformer(d_model=8, nhead=2, num_layers=2, dim_ff=2, max_len=64, dropout=0.1)"
      ],
      "metadata": {
        "id": "JiLkIR9TW8-x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MathTransformer().to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "model.train()\n",
        "steps = 800\n",
        "for step in range(1, steps+1):\n",
        "    x, y = make_batch(3, batch_size=1024)  # x,y: (N, T)\n",
        "    logits = model(x)                  # (N, T, V)\n",
        "    loss = criterion(logits.reshape(-1, len(TOKENS)), y.reshape(-1))\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if step % 100 == 0:\n",
        "        print(f\"step {step:4d} | loss {loss.item():.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ha9hqGvYaes",
        "outputId": "4e4f53fe-62b3-4773-d8a0-738ccea9b153"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step  100 | loss 0.8786\n",
            "step  200 | loss 0.6949\n",
            "step  300 | loss 0.5760\n",
            "step  400 | loss 0.5046\n",
            "step  500 | loss 0.4616\n",
            "step  600 | loss 0.4380\n",
            "step  700 | loss 0.4215\n",
            "step  800 | loss 0.4156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_prompt(s):\n",
        "    token_ids = encode(s)\n",
        "    if '=' in s:\n",
        "        token_ids = token_ids[:s.index('=')+2]\n",
        "        assert token_ids[-1] == TOKEN_MAP['=']\n",
        "\n",
        "    return torch.tensor([token_ids], dtype=torch.long, device=device)"
      ],
      "metadata": {
        "id": "X6Cb_bn6ZpGZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode(generate_instance(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2xVS4bUOatjr",
        "outputId": "14a9eb27-898b-4e64-8efe-685109656b63"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos>(2+(4+2))=(2+6)=8<eos>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_example(*args, verbose=True, **kwargs):\n",
        "    model.eval()\n",
        "\n",
        "    target_token_ids = generate_instance(*args, **kwargs)\n",
        "    target = decode(target_token_ids)\n",
        "\n",
        "    prompt = target[:target.index('=')+1]\n",
        "    prompt_token_ids = encode(prompt, eos=False)\n",
        "    prompt_batch = prompt_token_ids.reshape(shape=(1,-1))\n",
        "\n",
        "    actual_token_ids = model.generate(prompt_batch, max_new_tokens=25)[0]\n",
        "    actual = decode(actual_token_ids)\n",
        "\n",
        "    correct = actual == target\n",
        "\n",
        "    if verbose or not correct:\n",
        "        print(\"PROMPT\", decode(prompt_token_ids), \"TARGET\", target, \"ACTUAL\", actual, \"CORRECT\", correct)\n",
        "\n",
        "    return correct\n",
        "\n",
        "test_example(n=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz7VKQX2cTGk",
        "outputId": "26d9d8c3-bd76-4007-9d2c-dbbe783286ee"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROMPT <bos>(8+(1+4))= TARGET <bos>(8+(1+4))=(8+5)=13<eos> ACTUAL <bos>(8+(1+4))=(8+5)=13<eos> CORRECT True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    test_example(n=3, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkr_nJYhfa1R",
        "outputId": "d970d422-8d07-48d0-a0bf-59be66a7958b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROMPT <bos>((3+3)+2)= TARGET <bos>((3+3)+2)=(6+2)=8<eos> ACTUAL <bos>((3+3)+2)=(6+2)=8<eos> CORRECT True\n",
            "PROMPT <bos>(3+(6+6))= TARGET <bos>(3+(6+6))=(3+12)=15<eos> ACTUAL <bos>(3+(6+6))=(3+12)=15<eos> CORRECT True\n",
            "PROMPT <bos>(7+(3+2))= TARGET <bos>(7+(3+2))=(7+5)=12<eos> ACTUAL <bos>(7+(3+2))=(7+5)=12<eos> CORRECT True\n",
            "PROMPT <bos>((8+5)+8)= TARGET <bos>((8+5)+8)=(13+8)=21<eos> ACTUAL <bos>((8+5)+8)=(13+8)=21<eos> CORRECT True\n",
            "PROMPT <bos>(7+(4+6))= TARGET <bos>(7+(4+6))=(7+10)=17<eos> ACTUAL <bos>(7+(4+6))=(7+10)=17<eos> CORRECT True\n",
            "PROMPT <bos>((3+3)+7)= TARGET <bos>((3+3)+7)=(6+7)=13<eos> ACTUAL <bos>((3+3)+7)=(6+7)=13<eos> CORRECT True\n",
            "PROMPT <bos>((4+4)+8)= TARGET <bos>((4+4)+8)=(8+8)=16<eos> ACTUAL <bos>((4+4)+8)=(8+8)=16<eos> CORRECT True\n",
            "PROMPT <bos>((5+3)+9)= TARGET <bos>((5+3)+9)=(8+9)=17<eos> ACTUAL <bos>((5+3)+9)=(8+9)=17<eos> CORRECT True\n",
            "PROMPT <bos>((6+9)+2)= TARGET <bos>((6+9)+2)=(15+2)=17<eos> ACTUAL <bos>((6+9)+2)=(15+2)=17<eos> CORRECT True\n",
            "PROMPT <bos>((4+9)+4)= TARGET <bos>((4+9)+4)=(13+4)=17<eos> ACTUAL <bos>((4+9)+4)=(13+4)=17<eos> CORRECT True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    test_example(n=4, verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05CPDz7GfiM4",
        "outputId": "c03862ee-48db-451b-bbff-c48792c55943"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROMPT <bos>((8+9)+(4+1))= TARGET <bos>((8+9)+(4+1))=(17+5)=22<eos> ACTUAL <bos>((8+9)+(4+1))=(1919<eos> CORRECT False\n",
            "PROMPT <bos>(((9+2)+1)+9)= TARGET <bos>(((9+2)+1)+9)=((11+1)+9)=(12+9)=21<eos> ACTUAL <bos>(((9+2)+1)+9)=(1<eos> CORRECT False\n",
            "PROMPT <bos>((8+7)+(9+7))= TARGET <bos>((8+7)+(9+7))=(15+16)=31<eos> ACTUAL <bos>((8+7)+(9+7))=2)=23<eos> CORRECT False\n",
            "PROMPT <bos>(((8+3)+7)+7)= TARGET <bos>(((8+3)+7)+7)=((11+7)+7)=(18+7)=25<eos> ACTUAL <bos>(((8+3)+7)+7)=(19+9<eos> CORRECT False\n",
            "PROMPT <bos>(((1+2)+1)+8)= TARGET <bos>(((1+2)+1)+8)=((3+1)+8)=(4+8)=12<eos> ACTUAL <bos>(((1+2)+1)+8)=)=8<eos> CORRECT False\n",
            "PROMPT <bos>((4+2)+(9+5))= TARGET <bos>((4+2)+(9+5))=(6+14)=20<eos> ACTUAL <bos>((4+2)+(9+5))=(8<eos> CORRECT False\n",
            "PROMPT <bos>((8+8)+(2+5))= TARGET <bos>((8+8)+(2+5))=(16+7)=23<eos> ACTUAL <bos>((8+8)+(2+5))=(19+9<eos> CORRECT False\n",
            "PROMPT <bos>(8+(5+(3+6)))= TARGET <bos>(8+(5+(3+6)))=(8+(5+9))=(8+14)=22<eos> ACTUAL <bos>(8+(5+(3+6)))=(915<eos> CORRECT False\n",
            "PROMPT <bos>(((7+6)+4)+1)= TARGET <bos>(((7+6)+4)+1)=((13+4)+1)=(17+1)=18<eos> ACTUAL <bos>(((7+6)+4)+1)=(9<eos> CORRECT False\n",
            "PROMPT <bos>((3+(7+3))+6)= TARGET <bos>((3+(7+3))+6)=((3+10)+6)=(13+6)=19<eos> ACTUAL <bos>((3+(7+3))+6)=(9<eos> CORRECT False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benchmark your model\n",
        "\n",
        "Test your code with different numbers of integers and numbers of input digits.\n",
        "The `generate_instance` function provided uses the parameter `n` to control the number of integers, and `value_min` and `value_max` to control the range of integers.\n",
        "For example, 2 input digits would correspond to `value_min=10` and `value_max=99`.\n",
        "\n",
        "Test the accuracy on the combinations specified in the table below, and fill in your accuracy numbers in that table.\n",
        "Make sure that you run enough samples for statistical significance (usually at least 1000 recommended) as your benchmarking accuracy will be checked for consistency with tests by the auto-grader."
      ],
      "metadata": {
        "id": "YXx-zTa67RFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CHANGES HERE"
      ],
      "metadata": {
        "id": "QZQ08lQY7VaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in this table.\n",
        "\n",
        "| n | input digits | accuracy |\n",
        "|---|---|-----|\n",
        "| 2 | 1 | TODO |\n",
        "| 2 | 2 | TODO |\n",
        "| 2 | 3 | TODO |\n",
        "| 3 | 1 | TODO |\n",
        "| 3 | 2 | TODO |\n",
        "| 3 | 3 | TODO |\n",
        "| 4 | 1 | TODO |\n",
        "| 4 | 2 | TODO |\n",
        "| 5 | 1 | TODO |\n",
        "| 5 | 2 | TODO |\n",
        "\n",
        "Do not change the table header as the auto-grader will use it to check your results.\n"
      ],
      "metadata": {
        "id": "ArWZ5Bvi7W2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model and implement a command line interface.\n",
        "\n",
        "Your model will be tested automatically with a suite of examples with different numbers of values and digits matching your previous benchmark task.\n",
        "For this testing, you must save your model weights and write a program to run your model."
      ],
      "metadata": {
        "id": "iWVJhMqX6mjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save your model weights.\n",
        "\n",
        "Save your model weights as `math.pt` to be submitted in Gradescope."
      ],
      "metadata": {
        "id": "xHg1-iYG9rR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "Lo9huDRC-XYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write a program to run your model.\n",
        "\n",
        "Write a Python script `predict.py` that takes a single filename as input, reads each line as a prompt, generates the completion, and writes out the result to standard output.\n",
        "We will invoke your program with a command like `python3 predict.py INPUT.txt` and capture the standard output for grading.\n",
        "\n",
        "The input file will not include the special tokens such as `<bos>` or `<eos>`.\n",
        "Similarly, your output should not include them either.\n",
        "\n",
        "For example, given an input file with the following contents,\n",
        "```\n",
        "(((1+2)+1)+8)=\n",
        "```\n",
        "your program should write the following output.\n",
        "```\n",
        "(((1+2)+1)+8)=((3+1)+8)=(4+8)=12\n",
        "```\n"
      ],
      "metadata": {
        "id": "9xj9cXLu-Y0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Submission\n",
        "\n",
        "Submit your copy of this notebook with all your code, your saved model \"math.pt\", and your prediction script \"predict.py\" to Gradescope.\n"
      ],
      "metadata": {
        "id": "4t215yxs83-n"
      }
    }
  ]
}
